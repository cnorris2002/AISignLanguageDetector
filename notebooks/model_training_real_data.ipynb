{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e04fdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported landmark extractor function\n",
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier # First model to try out\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,  classification_report\n",
    "\n",
    "# Importing the function from Gabe's part (landmark_extractor)\n",
    "# path to file: \"\"\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "try: \n",
    "    from landmark_extractor import extract_normalized_landmarks\n",
    "    print(\"Successfully imported landmark extractor function\")\n",
    "except ImportError:\n",
    "    print(\"Error: Could not import from source. Make sure file exists or correct directory\")\n",
    "    extract_normalized_landmarks = None\n",
    "\n",
    "print (\"Libraries imported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c802954f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constants defined.\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = '../data/raw/ASL_Alphabet_dataset/'\n",
    "PROCESSED_DATA_DIR = '../data/processed/'\n",
    "X_SAVE_PATH = os.path.join(PROCESSED_DATA_DIR, 'X_landmarks.npy')\n",
    "Y_SAVE_PATH = os.path.join(PROCESSED_DATA_DIR, 'y_labels.npy')\n",
    "CLASS_NAMES_PATH = os.path.join(PROCESSED_DATA_DIR, 'class_names.npy') \n",
    "MODEL_SAVE_PATH = '../models/asl_classifier.pkl'\n",
    "\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)\n",
    "\n",
    "# Our model choice\n",
    "MODEL_CHOICE = RandomForestClassifier\n",
    "MODEL_PARAMS = {'n_estimators': 100, 'random_state': 42, 'n_jobs': -1} \n",
    "\n",
    "TEST_SPLIT_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "FORCE_REPROCESS_DATA = False \n",
    "\n",
    "print(\"Constants defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1826ab85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed data from ../data/processed/...\n",
      "Loaded X shape: (1245, 63), y shape: (1245,)\n",
      "Class names: ['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R'\n",
      " 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z']\n"
     ]
    }
   ],
   "source": [
    "X = None\n",
    "y = None\n",
    "class_names = None\n",
    "\n",
    "if not FORCE_REPROCESS_DATA and os.path.exists(X_SAVE_PATH) and os.path.exists(Y_SAVE_PATH) and os.path.exists(CLASS_NAMES_PATH):\n",
    "    print(f\"Loading pre-processed data from {PROCESSED_DATA_DIR}...\")\n",
    "    try:\n",
    "        X = np.load(X_SAVE_PATH)\n",
    "        y = np.load(Y_SAVE_PATH)\n",
    "        class_names = np.load(CLASS_NAMES_PATH)\n",
    "        print(f\"Loaded X shape: {X.shape}, y shape: {y.shape}\")\n",
    "        print(f\"Class names: {class_names}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading .npy files: {e}. Will reprocess data.\")\n",
    "        FORCE_REPROCESS_DATA = True \n",
    "\n",
    "# If data wasn't loaded, process it\n",
    "if X is None or y is None or class_names is None or FORCE_REPROCESS_DATA:\n",
    "    print(f\"Processing dataset from: {DATASET_PATH}\")\n",
    "    if extract_normalized_landmarks is None:\n",
    "        print(\"ERROR: Landmark extractor function not available. Cannot process data.\")\n",
    "    else:\n",
    "        X_data = []\n",
    "        y_data = []\n",
    "\n",
    "        try:\n",
    "            class_names = sorted([d for d in os.listdir(DATASET_PATH) if os.path.isdir(os.path.join(DATASET_PATH, d))])\n",
    "            print(f\"Found classes: {class_names}\")\n",
    "            start_time = time.time()\n",
    "            processed_count = 0\n",
    "\n",
    "            for label_index, class_name in enumerate(class_names):\n",
    "                class_path = os.path.join(DATASET_PATH, class_name)\n",
    "                print(f\"Processing class: {class_name} ({label_index+1}/{len(class_names)})...\")\n",
    "                image_files = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "                for image_name in image_files:\n",
    "                    image_path = os.path.join(class_path, image_name)\n",
    "                    # Call Gabe's function\n",
    "                    landmarks_vector = extract_normalized_landmarks(image_path) \n",
    "\n",
    "                    if landmarks_vector is not None:\n",
    "                        X_data.append(landmarks_vector)\n",
    "                        y_data.append(label_index) \n",
    "                        processed_count += 1\n",
    "                        if processed_count % 1000 == 0:\n",
    "                             print(f\"  Processed {processed_count} images...\")\n",
    "\n",
    "            end_time = time.time()\n",
    "            print(f\"\\nDataset processing complete. Extracted landmarks from {processed_count} images in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "            # Convert lists to NumPy arrays\n",
    "            X = np.array(X_data)\n",
    "            y = np.array(y_data)\n",
    "            class_names = np.array(class_names)\n",
    "\n",
    "            # Save the processed data\n",
    "            print(f\"Saving processed data to {PROCESSED_DATA_DIR}...\")\n",
    "            np.save(X_SAVE_PATH, X)\n",
    "            np.save(Y_SAVE_PATH, y)\n",
    "            np.save(CLASS_NAMES_PATH, class_names)\n",
    "            print(\"Processed data saved.\")\n",
    "            print(f\"Final X shape: {X.shape}\") \n",
    "            print(f\"Final y shape: {y.shape}\") \n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: Dataset path not found during processing: {DATASET_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during data processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987f9798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting real data...\n",
      "Real Train set size: 996 samples\n",
      "Real Validation set size: 249 samples\n",
      "\n",
      "Initializing and training model on real data...\n",
      "Using model: RandomForestClassifier(n_jobs=-1, random_state=42)\n",
      "Model training finished in 0.47 seconds.\n",
      "\n",
      "Evaluating model on real validation data...\n",
      "Validation Accuracy: 0.9960\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      1.00      1.00        10\n",
      "           B       1.00      1.00      1.00        10\n",
      "           C       1.00      1.00      1.00        10\n",
      "           D       1.00      1.00      1.00        10\n",
      "           E       1.00      1.00      1.00        10\n",
      "           F       1.00      1.00      1.00        10\n",
      "           G       1.00      1.00      1.00        10\n",
      "           H       1.00      1.00      1.00        10\n",
      "           I       1.00      1.00      1.00        10\n",
      "           J       1.00      1.00      1.00         9\n",
      "           K       1.00      1.00      1.00        10\n",
      "           L       1.00      0.90      0.95        10\n",
      "           M       1.00      1.00      1.00         4\n",
      "           N       1.00      1.00      1.00         6\n",
      "           O       1.00      1.00      1.00        10\n",
      "           P       1.00      1.00      1.00        10\n",
      "           Q       1.00      1.00      1.00        10\n",
      "           R       1.00      1.00      1.00        10\n",
      "           S       1.00      1.00      1.00        10\n",
      "           T       1.00      1.00      1.00        10\n",
      "           U       1.00      1.00      1.00        10\n",
      "           V       1.00      1.00      1.00        10\n",
      "           W       1.00      1.00      1.00        10\n",
      "           X       1.00      1.00      1.00        10\n",
      "           Y       1.00      1.00      1.00        10\n",
      "           Z       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           1.00       249\n",
      "   macro avg       1.00      1.00      1.00       249\n",
      "weighted avg       1.00      1.00      1.00       249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if X is not None and y is not None:\n",
    "    print(\"\\nSplitting real data...\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, \n",
    "        test_size=TEST_SPLIT_SIZE, \n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y \n",
    "    )\n",
    "    print(f\"Real Train set size: {X_train.shape[0]} samples\")\n",
    "    print(f\"Real Validation set size: {X_val.shape[0]} samples\")\n",
    "\n",
    "    print(\"\\nInitializing and training model on real data...\")\n",
    "    model = MODEL_CHOICE(**MODEL_PARAMS)\n",
    "    print(f\"Using model: {model}\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    print(f\"Model training finished in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    print(\"\\nEvaluating model on real validation data...\")\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    target_names = class_names if class_names is not None else None\n",
    "    report = classification_report(y_val, y_pred, target_names=target_names, zero_division=0)\n",
    "    print(report)\n",
    "\n",
    "    # Could add a confusion matrix if there's enough time\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping training and evaluation as real data was not loaded/processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c008d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving the trained model to: ../models/asl_classifier.pkl\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "if 'model' in locals() and X is not None:\n",
    "     print(f\"\\nSaving the trained model to: {MODEL_SAVE_PATH}\")\n",
    "     try:\n",
    "         joblib.dump(model, MODEL_SAVE_PATH)\n",
    "         print(\"Model saved successfully.\")\n",
    "     except Exception as e:\n",
    "         print(f\"Error saving model: {e}\")\n",
    "else:\n",
    "     print(\"\\nModel not trained/saved as data was not available.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
